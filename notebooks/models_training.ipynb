{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from models.train_full import train, test\n",
    "from models.models import CNNClassifier\n",
    "from models.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(ld, indent=0):\n",
    "    with open('result.txt', 'w', encoding='utf-8') as file:\n",
    "        for d in tqdm(ld):\n",
    "            file.write('{' + '\\n')\n",
    "            for key, value in d.items():\n",
    "                file.write('\\t' * (indent+1) + str(key) + ':' + str(value) + '\\n')\n",
    "                # file.write('\\t' * (indent+1) + str(key) + '\\n')\n",
    "                # file.write('\\t' * (indent+2) + str(value) + '\\n')\n",
    "            file.write('},\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "\n",
    "seed = 4444\n",
    "\n",
    "metric_filter_1 = 'val_mcc'\n",
    "metric_filter_2 = 'val_mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model = dict(\n",
    "    # dictionary with model information\n",
    "    in_channels=[3],\n",
    "    out_channels=[2],\n",
    "    # dim_layers=[[32, 64, 128], [16, 32, 64]],\n",
    "    dim_layers=[[32, 64, 128]],\n",
    "    # block_conv_layers=[3, 5],\n",
    "    block_conv_layers=[3],\n",
    "    residual=[True],\n",
    "    # max_pooling=[True, False],\n",
    "    max_pooling=[True],\n",
    "    transforms=[\n",
    "        (\n",
    "            '0',\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.RandomResizedCrop(size=(400, 400)),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            torchvision.transforms.Resize(size=(400, 400)),\n",
    "            # transforms.RandomResizedCrop(size=(400, 400)),\n",
    "            # test varios times over the same data and choose the mean\n",
    "            False,\n",
    "        ),\n",
    "        (\n",
    "            '1',\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(size=(400, 400)),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            torchvision.transforms.Resize(size=(400, 400)),\n",
    "            True,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "list_model = [dict(zip(dict_model.keys(), k)) for k in itertools.product(*dict_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        set_seed(seed)\n",
    "        \n",
    "        d = d.copy()\n",
    "        transforms = d.pop('transforms')\n",
    "\n",
    "        train(\n",
    "            model=CNNClassifier(**d),\n",
    "            dict_model=d,\n",
    "            log_dir=\"./logs_full\",\n",
    "            data_path=\"../data_full\",\n",
    "            save_path=\"./models/saved_full\",\n",
    "            lr=1e-2,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=150,\n",
    "            batch_size=32,\n",
    "            num_workers=2,\n",
    "            scheduler_mode='min_mse',\n",
    "            debug_mode=False,\n",
    "            device=None,\n",
    "            steps_save=15,\n",
    "            use_cpu=False,\n",
    "            loss_age_weight=1e-2,\n",
    "            scheduler_patience=30,\n",
    "            train_transforms=transforms[1],\n",
    "            test_transforms=transforms[2],\n",
    "            suffix=transforms[0],\n",
    "            use_cache=transforms[3],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [54:07<00:00, 162.37s/it] \n"
     ]
    }
   ],
   "source": [
    "res_test = test(\n",
    "    data_path = \"../data_full\",\n",
    "    save_path = './models/saved_full',\n",
    "    n_runs = 1,\n",
    "    batch_size = 64,\n",
    "    num_workers = 0,\n",
    "    debug_mode = False,\n",
    "    use_cpu = False,\n",
    "    save = True,\n",
    "    verbose = False,\n",
    "    transforms=torchvision.transforms.Resize(size=(400, 400)),\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_channels': 3,\n",
       " 'out_channels': 2,\n",
       " 'dim_layers': [32, 64, 128],\n",
       " 'block_conv_layers': 3,\n",
       " 'residual': True,\n",
       " 'max_pooling': True,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_batch_size': 32,\n",
       " 'train_scheduler_mode': 'min_mse',\n",
       " 'train_loss_age_weight': 0.01,\n",
       " 'train_suffix': '1',\n",
       " 'epoch': 135,\n",
       " 'train_loss': 0.7180283,\n",
       " 'val_mse_age': 131.51015,\n",
       " 'train_acc': 0.9394927024841309,\n",
       " 'val_acc': 0.8674965500831604,\n",
       " 'val_mcc': 0.7339866991692735,\n",
       " 'model_class': 'cnn',\n",
       " 'path_name': '3_2_[32_64_128]_3_True_True_0.01_adamw_32_min_mse_0.01_1_135',\n",
       " 'train_mae': 5.088548,\n",
       " 'val_mae': 8.730115,\n",
       " 'test_mae': 8.740539,\n",
       " 'train_mse': 42.900738,\n",
       " 'val_mse': 131.44864,\n",
       " 'test_mse': 136.33107,\n",
       " 'train_mcc': 0.8789576593001115,\n",
       " 'test_mcc': 0.7426456711701096,\n",
       " 'test_acc': 0.8714404106140137}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = res_test\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_channels': 3,\n",
       " 'out_channels': 2,\n",
       " 'dim_layers': [32, 64, 128],\n",
       " 'block_conv_layers': 3,\n",
       " 'residual': True,\n",
       " 'max_pooling': True,\n",
       " 'train_lr': 0.01,\n",
       " 'train_optimizer_name': 'adamw',\n",
       " 'train_batch_size': 32,\n",
       " 'train_scheduler_mode': 'min_mse',\n",
       " 'train_loss_age_weight': 0.01,\n",
       " 'train_suffix': '1',\n",
       " 'epoch': 120,\n",
       " 'train_loss': 0.88590217,\n",
       " 'val_mse_age': 127.54852,\n",
       " 'train_acc': 0.9190470576286316,\n",
       " 'val_acc': 0.8652835488319397,\n",
       " 'val_mcc': 0.7305876856123024,\n",
       " 'model_class': 'cnn',\n",
       " 'path_name': '3_2_[32_64_128]_3_True_True_0.01_adamw_32_min_mse_0.01_1_120',\n",
       " 'train_mae': 5.7857547,\n",
       " 'val_mae': 8.650999,\n",
       " 'test_mae': 8.628361,\n",
       " 'train_mse': 56.060917,\n",
       " 'val_mse': 127.48625,\n",
       " 'test_mse': 133.53343,\n",
       " 'train_mcc': 0.8400896585159275,\n",
       " 'test_mcc': 0.7315739561738387,\n",
       " 'test_acc': 0.8645285964012146}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model = dict(\n",
    "    # dictionary with model information\n",
    "    in_channels=[3],\n",
    "    out_channels=[2],\n",
    "    dim_layers=[[32, 64, 128], [16, 32, 64]],\n",
    "    block_conv_layers=[3, 5],\n",
    "    residual=[True],\n",
    "    max_pooling=[True, False],\n",
    "    transforms=[\n",
    "        (\n",
    "            '1',\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(size=(400, 400)),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            torchvision.transforms.Resize(size=(400, 400)),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "list_model = [dict(zip(dict_model.keys(), k)) for k in itertools.product(*dict_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "in_channels=3/out_channels=2/dim_layers=[32/64/128]/block_conv_layers=3/residual=True/max_pooling=True/train_lr=0.01/train_optimizer_name=adamw/train_batch_size=32/train_scheduler_mode=min_mse/train_loss_age_weight=0.01/train_suffix=1 -> 160.55604553222656: 100%|██████████| 200/200 [21:37:10<00:00, 389.15s/it]\n",
      " 12%|█▎        | 1/8 [21:37:10<151:20:12, 77830.30s/it]"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        set_seed(seed)\n",
    "        \n",
    "        d = d.copy()\n",
    "        transforms = d.pop('transforms')\n",
    "\n",
    "        train(\n",
    "            model=CNNClassifier(**d),\n",
    "            dict_model=d,\n",
    "            log_dir=\"./logs_full_2\",\n",
    "            data_path=\"../data_full\",\n",
    "            save_path=\"./models/saved_full_2\",\n",
    "            lr=1e-2,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=200,\n",
    "            batch_size=32,\n",
    "            num_workers=3,\n",
    "            scheduler_mode='min_mse',\n",
    "            debug_mode=False,\n",
    "            device=None,\n",
    "            steps_save=15,\n",
    "            use_cpu=False,\n",
    "            loss_age_weight=1e-2,\n",
    "            scheduler_patience=30,\n",
    "            train_transforms=transforms[1],\n",
    "            test_transforms=transforms[2],\n",
    "            suffix=transforms[0],\n",
    "            use_cache=transforms[3],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "res_test = test(\n",
    "    data_path = \"../data_full\",\n",
    "    save_path = './models/saved_full',\n",
    "    n_runs = 1,\n",
    "    batch_size = 64,\n",
    "    num_workers = 0,\n",
    "    debug_mode = False,\n",
    "    use_cpu = False,\n",
    "    save = True,\n",
    "    verbose = False,\n",
    "    transforms=torchvision.transforms.Resize(size=(400, 400)),\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_test\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_test[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model = dict(fsd\n",
    "    # dictionary with model information\n",
    "    in_channels=[3],\n",
    "    out_channels=[2],\n",
    "    dim_layers=[[32, 64, 128], [16, 32, 64]],\n",
    "    block_conv_layers=[3, 5],\n",
    "    residual=[True],\n",
    "    max_pooling=[True, False],\n",
    "    transforms=[\n",
    "        (\n",
    "            '1',\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(size=(200, 200)),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            torchvision.transforms.Resize(size=(200, 200)),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "list_model = [dict(zip(dict_model.keys(), k)) for k in itertools.product(*dict_model.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "in_channels=3/out_channels=2/dim_layers=[32/64/128]/block_conv_layers=3/residual=True/max_pooling=True/train_lr=0.01/train_optimizer_name=adamw/train_batch_size=32/train_scheduler_mode=min_mse/train_loss_age_weight=0.01/train_suffix=1 -> :   0%|          | 0/200 [00:06<?, ?it/s]\n",
      "  0%|          | 0/16 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\AgeGenderDetector\\notebooks\\models_training.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=4'>5</a>\u001b[0m d \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=5'>6</a>\u001b[0m transforms \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mtransforms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=7'>8</a>\u001b[0m train(\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39;49mCNNClassifier(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49md),\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=9'>10</a>\u001b[0m     dict_model\u001b[39m=\u001b[39;49md,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=10'>11</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./logs_full_2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=11'>12</a>\u001b[0m     data_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../data_full\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=12'>13</a>\u001b[0m     save_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./models/saved_full_2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=13'>14</a>\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=14'>15</a>\u001b[0m     optimizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madamw\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=15'>16</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=16'>17</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=17'>18</a>\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=18'>19</a>\u001b[0m     scheduler_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin_mse\u001b[39;49m\u001b[39m'\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=19'>20</a>\u001b[0m     debug_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=20'>21</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=21'>22</a>\u001b[0m     steps_save\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=22'>23</a>\u001b[0m     use_cpu\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=23'>24</a>\u001b[0m     loss_age_weight\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=24'>25</a>\u001b[0m     scheduler_patience\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=25'>26</a>\u001b[0m     train_transforms\u001b[39m=\u001b[39;49mtransforms[\u001b[39m1\u001b[39;49m],\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=26'>27</a>\u001b[0m     test_transforms\u001b[39m=\u001b[39;49mtransforms[\u001b[39m2\u001b[39;49m],\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=27'>28</a>\u001b[0m     suffix\u001b[39m=\u001b[39;49mtransforms[\u001b[39m0\u001b[39;49m],\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=28'>29</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49mtransforms[\u001b[39m3\u001b[39;49m],\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/models_training.ipynb#ch0000013?line=29'>30</a>\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\AgeGenderDetector\\notebooks\\..\\models\\train_full.py:159\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dict_model, log_dir, data_path, save_path, lr, optimizer_name, n_epochs, batch_size, num_workers, scheduler_mode, debug_mode, device, steps_save, use_cpu, loss_age_weight, scheduler_patience, train_transforms, test_transforms, suffix, use_cache)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=156'>157</a>\u001b[0m \u001b[39m# Start training: train mode and freeze bert\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=157'>158</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=158'>159</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, age, gender \u001b[39min\u001b[39;00m loader_train:\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=159'>160</a>\u001b[0m     \u001b[39m# To device\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=160'>161</a>\u001b[0m     img, age, gender \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device), age\u001b[39m.\u001b[39mto(device), gender\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/notebooks/../models/train_full.py?line=162'>163</a>\u001b[0m     \u001b[39m# Compute loss and update parameters\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\AgeGenderDetector\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=365'>366</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=366'>367</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=367'>368</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\AgeGenderDetector\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:314\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=311'>312</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=312'>313</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=313'>314</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vibal\\PycharmProjects\\AgeGenderDetector\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:927\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=919'>920</a>\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=920'>921</a>\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=921'>922</a>\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=922'>923</a>\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=923'>924</a>\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=924'>925</a>\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=925'>926</a>\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=926'>927</a>\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=927'>928</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/PycharmProjects/AgeGenderDetector/venv/lib/site-packages/torch/utils/data/dataloader.py?line=928'>929</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=117'>118</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=118'>119</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=119'>120</a>\u001b[0m _cleanup()\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=122'>123</a>\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/process.py?line=123'>124</a>\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=221'>222</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=222'>223</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=223'>224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=323'>324</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=324'>325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n",
      "\u001b[0;32m    <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=325'>326</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n",
      "\u001b[1;32m--> <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/context.py?line=326'>327</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/popen_spawn_win32.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/popen_spawn_win32.py?line=91'>92</a>\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n",
      "\u001b[1;32m---> <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/popen_spawn_win32.py?line=92'>93</a>\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/popen_spawn_win32.py?line=93'>94</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/popen_spawn_win32.py?line=94'>95</a>\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/reduction.py?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;32m     <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/reduction.py?line=58'>59</a>\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='file:///c%3A/Users/vibal/AppData/Local/Programs/Python/Python39/lib/multiprocessing/reduction.py?line=59'>60</a>\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for d in tqdm(list_model):\n",
    "        set_seed(seed)\n",
    "        \n",
    "        d = d.copy()\n",
    "        transforms = d.pop('transforms')\n",
    "\n",
    "        train(\n",
    "            model=CNNClassifier(**d),\n",
    "            dict_model=d,\n",
    "            log_dir=\"./logs_full_3\",\n",
    "            data_path=\"../data_full\",\n",
    "            save_path=\"./models/saved_full_3\",\n",
    "            lr=1e-2,\n",
    "            optimizer_name=\"adamw\",\n",
    "            n_epochs=200,\n",
    "            batch_size=32,\n",
    "            num_workers=3,\n",
    "            scheduler_mode='min_mse',\n",
    "            debug_mode=False,\n",
    "            device=None,\n",
    "            steps_save=15,\n",
    "            use_cpu=False,\n",
    "            loss_age_weight=1e-2,\n",
    "            scheduler_patience=30,\n",
    "            train_transforms=transforms[1],\n",
    "            test_transforms=transforms[2],\n",
    "            suffix=transforms[0],\n",
    "            use_cache=transforms[3],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "res_test = test(\n",
    "    data_path = \"../data_full\",\n",
    "    save_path = './models/saved_full',\n",
    "    n_runs = 1,\n",
    "    batch_size = 64,\n",
    "    num_workers = 0,\n",
    "    debug_mode = False,\n",
    "    use_cpu = False,\n",
    "    save = True,\n",
    "    verbose = False,\n",
    "    transforms=torchvision.transforms.Resize(size=(400, 400)),\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_test\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_1] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = res_test[2]\n",
    "# ascending order\n",
    "sort_idx = np.argsort([k['dict'][metric_filter_2] for k in all])[::-1]\n",
    "all[sort_idx[0]]['dict']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37be9487e307834247f9cc00a1ec46ceeb3f522b7edf17e3b2d74c6ce713e314"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
